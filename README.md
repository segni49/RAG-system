# ğŸ§  Naive RAG Chatbot - Ask about Agriculture

A lightweight, local-first Retrieval-Augmented Generation (RAG) chatbot built with LangChain, FAISS, HuggingFace embeddings, and Ollama-powered LLMs. It answers questions based on your own PDF documents â€” and gracefully says "I don't know" when asked something outside its context.

---

## ğŸš€ Features

- âœ… Loads and cleans multiple PDFs using PyPDF2
- âœ… Splits text into overlapping chunks for semantic search
- âœ… Embeds chunks using HuggingFace sentence transformers
- âœ… Stores embeddings in a FAISS vectorstore
- âœ… Answers questions using a local LLM (e.g., `tinyllama` via Ollama)
- âœ… Grounded generation: refuses to hallucinate answers
- âœ… Streamlit UI for interactive querying

---

## ğŸ“ Project Structure

```text
RAG-system/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_chatbot.py         # Streamlit frontend
â”œâ”€â”€ generation/
â”‚   â””â”€â”€ generate_answer.py           # RAG logic and LLM prompt
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ load_and_clean.py            # PDF loading and cleaning
â”‚   â”œâ”€â”€ chunk_text.py                # Text chunking
â”‚   â””â”€â”€ embed_store.py               # Embedding and FAISS storage
â”œâ”€â”€ retrieval/
â”‚   â””â”€â”€ retrieve_chunks.py           # Chunk retrieval for testing
â”œâ”€â”€ data/
â”‚   â””â”€â”€ *.pdf                        # Your source documents
â”œâ”€â”€ rag_project/
â”‚   â””â”€â”€ vectorstore/                 # FAISS index and metadata
â””â”€â”€ requirements.txt                 # Dependencies
```

---

## ğŸ› ï¸ Installation

### 1. Clone the repo

```bash
git clone https://github.com/your-username/rag-chatbot.git
cd rag-chatbot
```

### 2. Create a virtual environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ“š Usage

### Step 1: Prepare your PDFs

Place all your `.pdf` files inside the `data/` folder.

### Step 2: Generate the vectorstore

```bash
python ingestion/embed_store.py
```

This will create `rag_project/vectorstore/index.faiss` and `index.pkl`.

### Step 3: Launch the chatbot

```bash
streamlit run app/streamlit_chatbot.py
```

Ask questions based on your documents. If the answer isnâ€™t found, the bot will say:

> â€œI donâ€™t know. Thatâ€™s outside the context of the documents I was trained on.â€

---

## ğŸ§  Model Notes

- Embedding model: `sentence-transformers/paraphrase-MiniLM-L3-v2`
- LLM: `tinyllama` via [Ollama](https://ollama.com)
- Vectorstore: FAISS (local, fast, scalable)

---

## ğŸ§ª Testing Retrieval

You can test chunk retrieval directly:

```bash
python retrieval/retrieve_chunks.py
```

---

## ğŸ“¦ Deployment

This project is designed to run locally. For deployment to HuggingFace Spaces or Streamlit Cloud:

- Include `requirements.txt`
- Add a `README.md`
- Ensure FAISS index is pre-generated
- Use CPU-friendly models

---

## ğŸ™‹ FAQ

**Q: What happens if I ask something outside the PDFs?**  
A: The chatbot will respond with â€œI donâ€™t knowâ€¦â€ â€” no hallucination.

**Q: Can I use scanned PDFs?**  
A: PyPDF2 only works with text-based PDFs. For scanned documents, consider switching to `pdfplumber` or `UnstructuredPDFLoader`.

**Q: Can I use a different LLM?**  
A: Yes! Just change the model name in `generate_answer.py`.

---

## ğŸ“„ License

MIT License â€” free to use, modify, and share.

---

## âœ¨ Credits

Built by [Segni Abera](https://github.com/segni49)  
Powered by LangChain, HuggingFace, FAISS, and Ollama
